{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural SEED smORFs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiongjeffrey/challenge-iclr-2022/blob/master/Neural_SEED_smORFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuMJWX_7SgIB"
      },
      "source": [
        "# NeuroSEED in Small Open Reading Frame (smORF) Proteins\n",
        "\n",
        "## Introduction\n",
        "Recently, hyperbolic spaces have been shown to better represent data emerging from latent hierarchies in a variety of contexts (Chamberlain et al, 2017). One such hierarchy emerges among genetic sequences, which over the course of evolution, slowly diverge from one another but nevertheless share a common ancestor (Corso et al, 2021). One application that could benefit from hyperbolic embedding is in the detection of small proteins. Small proteins are responsible for a myriad of functions, including cell-cell communication, regulatory networks, and cell cycle (Gray et al, 2022). Small proteins are difficult to identify empirically due to their small size. However, a recent approach identified novel small protein families by detecting groups of many closely-related suspected sequences and using it to train a neural network (Durrant and Bhatt, 2021). Nevertheless, identifying similarity among sequences through distance calculation is a time-consuming task. Here, we embed small protein sequences in the hyperbolic space and use their distances to predict small protein sequences in an improved computational runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWhBfRtV0wm"
      },
      "source": [
        "# Implementation\n",
        "## Motivation\n",
        "Small Open Reading Frames (smORFs) are a diverse class of proteins that are used in cells across an enormous range of functions. They are defined not by their function, but rather by the size of the genetics sequence that encodes them. With existing methods, it is very difficult to accurately classify which smORFs accomplish which functions. In fact, over 90% of the small protein families have no known domain and almost half are not represented in reference genomes (Sberro et al).\n",
        "\n",
        "Although advances have been made at identifying which sequences can produce smORFs, it is still very difficult to identify in the computationally efficient manner which kind of smORF that a sequence may belong to. This is because existing methods largely use Euclidean or Levenshtein distance calculation between sequences, without accounting for the hierarchical nature that defines classes of smORFs. For example, two related smORFs share a common genetic ancestor, and thus the calculation for the edit distance between them can be simplified drastically. Hyperbolic embeddings have proven to be more efficient at dealing with hierarchical data, and we seek to use these characteristics to our advantage. \n",
        "\n",
        "Our goal with this project is to illuminate whether hyperbolic embeddings improve fidelity of clustering, enabling us to better identify the function of smORFs and consequently predict biological function from a sequence.\n",
        "\n",
        "## Neural Embeddings in Hyperbolic Space\n",
        "Neural SEED is a framework that embeds genetic sequences in a particular topology from which edit distance can be calculated.  Our method is to train the encoder against known smORF edit distance data calculated using Levenshtein distances in both Euclidean and hyperbolic spaces. In addition, we compare these values with other methods of calculating distance (square, Manhattan, and cosine) to validate our results.\n",
        "\n",
        "![Cover](https://raw.githubusercontent.com/gcorso/NeuroSEED/master/tutorial/cover.png)\n",
        "Figure 1: Encoder takes genetic sequences and maps them onto a topology from which edit distance can be calculated (courtesy of Corso et al.)\n",
        "\n",
        "We hypothesize that hyperbolic space embeddings will allow for more accurate calculation of edit distance because of the hierarchical nature of the underlying sequences. By training an encoder which can quickly and accurately predict distances between smORF sequences, we can more effectively understand the relationship between these sequences via hierarchical clustering. Hyperbolic spaces also have the additional benefit of portraying ancestral branching relationships quite well, which can provide us an understanding of how the evolutionary path to a given sequence may have emerged.\n",
        "\n",
        "## Datasets Used\n",
        "We used data from the SmORFinder dataset. This data consists of strings of genome sequences that represent various known, largely unclassified, smORFs. We subsetted the data into 3 datasets: strings_test (containing 100 sequences), strings_subset (containing 7000 sequences), and clean_strings (containing 35000 sequences). We calculated the Levenshtein Distance between each pair of smORF strings within each dataset. We used the Levenshtein Distance as standard in the calculation of distance between genetic strings. One major goal of creating our embedding is to develop an encoder that quick estimates the Levenshtein Distance between two sequences across datasets. The Levenshtein Distance calculation for the strings_test was about 0.0208 seconds, for strings_subset it was about 1150 seconds, and calculation for clean-strings took more than 10 hours.\n",
        "\n",
        "Post-processed strings_test and strings_subset datasets are both available on our github (clean_strings is too large to host), as well as the embedded data for reference. We roughly separated each dataset into 60% training, 20% validating, and 20% testing. \n",
        "\n",
        "## Other Research Considered\n",
        "Prior research attempts to classify smORFs using a Hidden Markov Model (Durrant and Bhatt, 2021). However, novel testing illustrates that deep learning models have greater accuracy compared to Hidden Markov Models (Durrant and Bhatt, 2021). However, such testing does not use strong data on smORFs (Sberro et al), instead using a simple thresholding function. The SmORFinder dataset is a more complete reflection of existing smORF data and is a much stronger representation of the current field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xztLd6M_ZpKn"
      },
      "source": [
        "\n",
        "# Code Architecture and Implementation\n",
        "We use the NeuroSEED model as the foundation of this work (Corso et al, 2020). Geomstats is a core component of the code, providing the necessary functions to accurately calculate hyperbolic distance via an implementation of the Poincare Ball. Our model uses a recurrent neural network to produce an encoder that has learned how to accurately estimate the Levenshtein Distance between two given sequences and embed them onto a given topology. The encoder utilizes one of five distance functions (cosine, Manhattan, square, Euclidean, and hyperbolic) to learn how to estimate the Levenshtein Distance between two sequences.\n",
        "\n",
        "We evaluate the efficacy of the encoder using a root-mean-square error between the embedded distance between two sequences and the real Levenshtein Distance calculated between them. The lower the error, the more accurate the encoder is at estimating Levenshtein Distances.\n",
        "\n",
        "Our code makes use of functions from the original NeuroSEED model (Corso et al, 2021) in addition to our own auxilliary functions. The full model is hosted at https://github.com/nongiga/NeuroSEED, whereas this notebook only contains a small portion of the code. \n",
        "\n",
        "We highly recommend running this code in VSCode for fidelity and ease of running, as the databases we use are quite large and the most accurate database cannot be hosted on GitHub (only locally)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyPzAuvycefH"
      },
      "source": [
        "Setting up the required packages. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install geomstats\n",
        "!apt install clustalw\n",
        "!pip install biopython\n",
        "!pip install python-Levenshtein\n",
        "!pip install Cython\n",
        "!pip install networkx\n",
        "!pip install tqdm\n",
        "!pip install gdown\n",
        "!pip install pytorch\n",
        "!pip install torch==1.10.0+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/gcorso/neural_seed.git\n",
        "\n",
        "!pip install tensorflow\n",
        "\n",
        "\n",
        "import os\n",
        "os.chdir(\"neural_seed\")\n",
        "!cd hierarchical_clustering/relaxed/mst; python setup.py build_ext --inplace; cd ../unionfind; python setup.py build_ext --inplace; cd ..; cd ..; cd ..;\n",
        "os.environ['GEOMSTATS_BACKEND'] = 'pytorch'\n",
        "\n",
        "!pip install keras\n",
        "\n",
        "!pip install -q torch==1.10.0 torchvision"
      ],
      "metadata": {
        "id": "nLzPJt1fMUUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCBrVVic4Xp"
      },
      "source": [
        "import torch\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "from edit_distance.train import load_edit_distance_dataset\n",
        "from util.data_handling.data_loader import get_dataloaders\n",
        "from util.ml_and_math.loss_functions import AverageMeter\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from scipy.stats import mode\n",
        "\n",
        "from edit_distance.task.dataset_generator_genomic import EditDistanceGenomicDatasetGenerator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from geomstats.geometry.poincare_ball import PoincareBall\n",
        "\n",
        "numpy_type_map = {\n",
        "     'float64': torch.DoubleTensor,\n",
        "     'float32': torch.FloatTensor,\n",
        "     'float16': torch.HalfTensor,\n",
        "     'int64': torch.LongTensor,\n",
        "     'int32': torch.IntTensor,\n",
        "     'int16': torch.ShortTensor,\n",
        "     'int8': torch.CharTensor,\n",
        "     'uint8': torch.ByteTensor,\n",
        " }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Generation\n",
        "\n",
        "This portion takes the longest of any part of the runtime. We recommend pulling directly from the GitHub for the strings_subset and strings_test datasets. You will need to run this code to produce the clean_strings dataset and store it locally."
      ],
      "metadata": {
        "id": "qtxL40Ld133x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/bhattlab/SupplementaryInformation/raw/master/SmORFinder/datasets.tar.gz\n",
        "!tar -zxvf datasets.tar.gz -C '/content/'\n",
        "\n",
        "df = pd.read_csv(\"/content/datasets/dataset_FINAL.tsv\", sep='\\t')\n",
        "df.head()\n",
        "\n",
        "# save subset of dataset\n",
        "\n",
        "df_subset=df.sample(10000, random_state=42)\n",
        "df_subset.to_csv('smorf_subset.csv')\n",
        "\n",
        "subset_groups={}\n",
        "\n",
        "subset_groups['clean_strings']=df['smorf'].str.contains(r'^[ACTG]+$', na=False)\n",
        "\n",
        "smorfams = df.clust[df.clust.str.startswith('smorfam') & df.y.str.fullmatch('positive')]\n",
        "md, count = mode(smorfams)\n",
        "subset_groups['largest_group_strings']=df.clust.str.startswith(md[0]) & df.y.str.fullmatch('positive') & subset_groups['clean_strings']\n",
        "\n",
        "for name, boo_list in subset_groups.items():\n",
        "    with open('/content/datasets/'+ name+'.txt', 'w') as f_out:\n",
        "        f_out.writelines(\"%s\\n\" % l for l in df[boo_list].smorf.values)\n",
        "\n",
        "\n",
        "# generate train-test-val splits for select datasets and pickle them\n",
        "subsets={\"strings_test\":100,\"strings_subset\":7000,\"clean_strings\":35000}\n",
        "for n, train_size in subsets.items():\n",
        "    parser = create_parser('/content/datasets/'+n+'.pkl','/content/datasets/clean_strings.txt',  train_size,round(train_size/10),round(train_size/5))\n",
        "    generate_datasets(parser)\n",
        "\n",
        "train_size=50\n",
        "parser=create_parser('/content/datasets/largest_group_strings.pkl','/content/datasets/largest_group_strings.txt',  train_size,round(train_size/10),round(train_size/5))\n",
        "generate_datasets(parser)"
      ],
      "metadata": {
        "id": "jVCd8bi32NbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the functions to calculate distances"
      ],
      "metadata": {
        "id": "eaIblwXG2Xk0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXBg45KBeACe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from geomstats.geometry.poincare_ball import PoincareBall\n",
        "\n",
        "def square_distance(t1_emb, t2_emb,scale=1):\n",
        "    D = t1_emb - t2_emb\n",
        "    d = torch.sum(D * D, dim=-1)\n",
        "    return d\n",
        "\n",
        "\n",
        "def euclidean_distance(t1_emb, t2_emb,scale=1):\n",
        "    D = t1_emb - t2_emb\n",
        "    d = torch.norm(D, dim=-1)\n",
        "    return d\n",
        "\n",
        "\n",
        "def cosine_distance(t1_emb, t2_emb,scale=1):\n",
        "    return 1 - nn.functional.cosine_similarity(t1_emb, t2_emb, dim=-1, eps=1e-6)\n",
        "\n",
        "\n",
        "def manhattan_distance(t1_emb, t2_emb,scale=1):\n",
        "    D = t1_emb - t2_emb\n",
        "    d = torch.sum(torch.abs(D), dim=-1)\n",
        "    return d\n",
        "\n",
        "def hyperbolic_geomstats_distance(u,v,scale=1):\n",
        "    return PoincareBall(u.size()[1]).metric.dist(u,v)\n",
        "    \n",
        "\n",
        "def hyperbolic_distance(u, v, epsilon=1e-7):  # changed from epsilon=1e-7 to reduce error\n",
        "    sqdist = torch.sum((u - v) ** 2, dim=-1)\n",
        "    squnorm = torch.sum(u ** 2, dim=-1)\n",
        "    sqvnorm = torch.sum(v ** 2, dim=-1)\n",
        "    x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + epsilon\n",
        "    z = torch.sqrt(x ** 2 - 1)\n",
        "    return torch.log(x + z)\n",
        "\n",
        "\n",
        "def hyperbolic_distance_numpy(u, v, epsilon=1e-9):\n",
        "    sqdist = np.sum((u - v) ** 2, axis=-1)\n",
        "    squnorm = np.sum(u ** 2, axis=-1)\n",
        "    sqvnorm = np.sum(v ** 2, axis=-1)\n",
        "    x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + epsilon\n",
        "    z = np.sqrt(x ** 2 - 1)\n",
        "    return np.log(x + z)\n",
        "\n",
        "\n",
        "DISTANCE_TORCH = {\n",
        "    'square': square_distance,\n",
        "    'euclidean': euclidean_distance,\n",
        "    'cosine': cosine_distance,\n",
        "    'manhattan': manhattan_distance,\n",
        "    'hyperbolic': hyperbolic_distance\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDa6aholkv4z"
      },
      "source": [
        "General training and evaluation routines used to train the models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_TS2a5VmQCQ"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from edit_distance.task.dataset import EditDistanceDatasetSampled, EditDistanceDatasetComplete,EditDistanceDatasetSampledCalculated\n",
        "from edit_distance.task.dataset import EditDistanceDatasetCompleteCalculated\n",
        "from edit_distance.models.hyperbolics import RAdam\n",
        "from edit_distance.models.pair_encoder import PairEmbeddingDistance\n",
        "from util.data_handling.data_loader import get_dataloaders\n",
        "from util.ml_and_math.loss_functions import MAPE\n",
        "from util.ml_and_math.loss_functions import AverageMeter\n",
        "\n",
        "\n",
        "def general_arg_parser():\n",
        "    \"\"\" Parsing of parameters common to all the different models \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data', type=str, default='../../data/edit_qiita_small.pkl', help='Dataset path')\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training (GPU)')\n",
        "    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
        "    parser.add_argument('--epochs', type=int, default=2, help='Number of epochs to train')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate')\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay')\n",
        "    parser.add_argument('--dropout', type=float, default=0.0, help='Dropout rate (1 - keep probability)')\n",
        "    parser.add_argument('--patience', type=int, default=50, help='Patience')\n",
        "    parser.add_argument('--print_every', type=int, default=1, help='Print training results every')\n",
        "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')\n",
        "    parser.add_argument('--embedding_size', type=int, default=5, help='Size of embedding')\n",
        "    parser.add_argument('--distance', type=str, default='hyperbolic', help='Type of distance to use')\n",
        "    parser.add_argument('--workers', type=int, default=0, help='Number of workers')\n",
        "    parser.add_argument('--loss', type=str, default=\"mse\", help='Loss function to use (mse, mape or mae)')\n",
        "    parser.add_argument('--plot', action='store_true', default=False, help='Plot real vs predicted distances')\n",
        "    parser.add_argument('--closest_data_path', type=str, default='', help='Dataset for closest string retrieval tests')\n",
        "    parser.add_argument('--hierarchical_data_path', type=str, default='', help='Dataset for hierarchical clustering')\n",
        "    parser.add_argument('--construct_msa_tree', type=str, default='False', help='Whether to construct NJ tree testset')\n",
        "    parser.add_argument('--extr_data_path', type=str, default='', help='Dataset for further edit distance tests')\n",
        "    parser.add_argument('--scaling', type=str, default='False', help='Project to hypersphere (for hyperbolic)')\n",
        "    parser.add_argument('--hyp_optimizer', type=str, default='Adam', help='Optimizer for hyperbolic (Adam or RAdam)')\n",
        "    return parser\n",
        "\n",
        "\n",
        "def execute_train(model_class, model_args, args):\n",
        "    # set device\n",
        "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "    device = 'cpu'\n",
        "    print('Using device:', device)\n",
        "\n",
        "    # set the random seed\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    # load data\n",
        "    datasets = load_edit_distance_dataset(args.data)\n",
        "    loaders = get_dataloaders(datasets, batch_size=args.batch_size, workers=args.workers)\n",
        "\n",
        "    # fix hyperparameters\n",
        "    model_args = SimpleNamespace(**model_args)\n",
        "    model_args.device = device\n",
        "    model_args.len_sequence = datasets['train'].len_sequence\n",
        "    model_args.embedding_size = args.embedding_size\n",
        "    model_args.dropout = args.dropout\n",
        "    print(\"Length of sequence\", datasets['train'].len_sequence)\n",
        "    args.scaling = True if args.scaling == 'True' else False\n",
        "\n",
        "    # generate model\n",
        "    embedding_model = model_class(**vars(model_args))\n",
        "    model = PairEmbeddingDistance(embedding_model=embedding_model, distance=args.distance, scaling=args.scaling)\n",
        "    model.to(device)\n",
        "\n",
        "    # select optimizer\n",
        "    if args.distance == 'hyperbolic' and args.hyp_optimizer == 'RAdam':\n",
        "        optimizer = RAdam(model.parameters(), lr=args.lr)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "    # select loss\n",
        "    loss = None\n",
        "    if args.loss == \"mse\":\n",
        "        loss = nn.MSELoss()\n",
        "    elif args.loss == \"mae\":\n",
        "        loss = nn.L1Loss()\n",
        "    elif args.loss == \"mape\":\n",
        "        loss = MAPE\n",
        "\n",
        "    # print total number of parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('Total params', total_params)\n",
        "\n",
        "    # Train model\n",
        "    t_total = time.time()\n",
        "    bad_counter = 0\n",
        "    best = 1e10\n",
        "    best_epoch = -1\n",
        "    start_epoch = 0\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        t = time.time()\n",
        "        loss_train = train(model, loaders['train'], optimizer, loss, device)\n",
        "        loss_val = test(model, loaders['val'], loss, device)\n",
        "\n",
        "        # print progress\n",
        "        if epoch % args.print_every == 0:\n",
        "            print('Epoch: {:04d}'.format(epoch + 1),\n",
        "                  'loss_train: {:.6f}'.format(loss_train),\n",
        "                  'loss_val: {:.6f} MAPE {:.4f}'.format(*loss_val),\n",
        "                  'time: {:.4f}s'.format(time.time() - t))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if loss_val[0] < best:\n",
        "            # save current model\n",
        "            torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "            # remove previous model\n",
        "            if best_epoch >= 0:\n",
        "                os.remove('{}.pkl'.format(best_epoch))\n",
        "            # update training variables\n",
        "            best = loss_val[0]\n",
        "            best_epoch = epoch\n",
        "            bad_counter = 0\n",
        "        else:\n",
        "            bad_counter += 1\n",
        "\n",
        "        if bad_counter == args.patience:\n",
        "            print('Early stop at epoch {} (no improvement in last {} epochs)'.format(epoch + 1, bad_counter))\n",
        "            break\n",
        "\n",
        "    print('Optimization Finished!')\n",
        "    print('Total time elapsed: {:.4f}s'.format(time.time() - t_total))\n",
        "\n",
        "    # Restore best model\n",
        "    print('Loading {}th epoch'.format(best_epoch + 1))\n",
        "    model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "    # Testing\n",
        "    for dset in loaders.keys():\n",
        "        if args.plot:\n",
        "            avg_loss = test_and_plot(model, loaders[dset], loss, device, dset)\n",
        "        else:\n",
        "            avg_loss = test(model, loaders[dset], loss, device)\n",
        "        print('Final results {}: loss = {:.6f}  MAPE {:.4f}'.format(dset, *avg_loss))\n",
        "\n",
        "    # Nearest neighbour retrieval\n",
        "    if args.closest_data_path != '':\n",
        "        print(\"Closest string retrieval\")\n",
        "        closest_string_testing(encoder_model=model, data_path=args.closest_data_path,\n",
        "                               batch_size=args.batch_size, device=device, distance=args.distance)\n",
        "\n",
        "    # Hierarchical clustering\n",
        "    if args.hierarchical_data_path != '':\n",
        "        print(\"Hierarchical clustering\")\n",
        "        hierarchical_clustering_testing(encoder_model=model, data_path=args.hierarchical_data_path,\n",
        "                                        batch_size=args.batch_size, device=device, distance=args.distance)\n",
        "\n",
        "    # MSA tree construction on test set\n",
        "    if args.construct_msa_tree == 'True':\n",
        "        print(\"MSA tree construction\")\n",
        "        approximate_guide_trees(encoder_model=model, dataset=datasets['test'],\n",
        "                                batch_size=args.batch_size, device=device, distance=args.distance)\n",
        "\n",
        "    # Extra datasets testing (e.g. extrapolation)\n",
        "    if args.extr_data_path != '':\n",
        "        print(\"Extra datasets testing\")\n",
        "        datasets = load_edit_distance_dataset(args.extr_data_path)\n",
        "        loaders = get_dataloaders(datasets, batch_size=max(1, args.batch_size // 8), workers=args.workers)\n",
        "\n",
        "        for dset in loaders.keys():\n",
        "            if args.plot:\n",
        "                avg_loss = test_and_plot(model, loaders[dset], loss, device, dset)\n",
        "            else:\n",
        "                avg_loss = test(model, loaders[dset], loss, device)\n",
        "            print('Final results {}: loss = {:.6f}  MAPE {:.4f}'.format(dset, *avg_loss))\n",
        "\n",
        "    torch.save((model_class, model_args, model.embedding_model.state_dict(), args.distance),\n",
        "               '{}.pkl'.format(model_class.__name__))\n",
        "\n",
        "\n",
        "def load_edit_distance_dataset(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        sequences, distances = pickle.load(f)\n",
        "\n",
        "    datasets = {}\n",
        "    for key in sequences.keys():\n",
        "        if len(sequences[key].shape) == 2:  # datasets without batches\n",
        "            if key == 'train':\n",
        "                datasets[key] = EditDistanceDatasetSampled(sequences[key].unsqueeze(0), distances[key].unsqueeze(0),\n",
        "                                                           multiplicity=10)\n",
        "            else:\n",
        "                datasets[key] = EditDistanceDatasetComplete(sequences[key], distances[key])\n",
        "        else:  # datasets with batches\n",
        "            datasets[key] = EditDistanceDatasetSampled(sequences[key], distances[key])\n",
        "    return datasets\n",
        "\n",
        "def load_edit_distance_dataset_calculate(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        sequences, distances = pickle.load(f)\n",
        "\n",
        "    datasets = {}\n",
        "    for key in sequences.keys():\n",
        "        if len(sequences[key].shape) == 2:  # datasets without batches\n",
        "            if key == 'train':\n",
        "                datasets[key] = EditDistanceDatasetSampledCalculated(sequences[key].unsqueeze(0), distances[key].unsqueeze(0),\n",
        "                                                           multiplicity=10)\n",
        "            else:\n",
        "                datasets[key] = EditDistanceDatasetCompleteCalculated(sequences[key], distances[key])\n",
        "        else:  # datasets with batches\n",
        "            datasets[key] = EditDistanceDatasetSampledCalculated(sequences[key], distances[key])\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def train(model, loader, optimizer, loss, device):\n",
        "    device = 'cpu'\n",
        "    avg_loss = AverageMeter()\n",
        "    model.train()\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        # sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            # forward propagation\n",
        "            optimizer.zero_grad()\n",
        "            output = model(sequences)\n",
        "\n",
        "            # loss and backpropagation\n",
        "            loss_train = loss(output, labels)\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # keep track of average loss\n",
        "        avg_loss.update(loss_train.data.item(), sequences.shape[0])\n",
        "\n",
        "    return avg_loss.avg\n",
        "\n",
        "\n",
        "def test(model, loader, loss, device):\n",
        "    avg_loss = AverageMeter()\n",
        "    model.eval()\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        # sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # forward propagation and loss computation\n",
        "        output = model(sequences)\n",
        "        loss_val = loss(output, labels).data.item()\n",
        "        avg_loss.update(loss_val, sequences.shape[0])\n",
        "\n",
        "    return avg_loss.avg\n",
        "\n",
        "\n",
        "def test_and_plot(model, loader, loss, device, dataset):\n",
        "    avg_loss = AverageMeter(len_tuple=2)\n",
        "    model.eval()\n",
        "\n",
        "    output_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for sequences, labels in loader:\n",
        "        # move examples to right device\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "\n",
        "        # forward propagation and loss computation\n",
        "        output = model(sequences)\n",
        "        loss_val = loss[dt](output, labels).data.item()\n",
        "        mape = MAPE(output, labels).data.item()\n",
        "        avg_loss.update((loss_val, mape), sequences.shape[0])\n",
        "\n",
        "        # append real and predicted distances to lists\n",
        "        output_list.append(output.cpu().detach().numpy())\n",
        "        labels_list.append(labels.cpu().detach().numpy())\n",
        "\n",
        "    # save real and predicted distances for offline plotting\n",
        "    outputs = np.concatenate(output_list, axis=0)\n",
        "    labels = np.concatenate(labels_list, axis=0)\n",
        "    pickle.dump((outputs, labels), open(dataset + \".pkl\", \"wb\"))\n",
        "    # plt.plot(outputs, labels, 'o', color='black')\n",
        "    # plt.show()\n",
        "\n",
        "    return avg_loss.avg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the models"
      ],
      "metadata": {
        "id": "8pFo0jRQ2-CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "#  Train my models\n",
        "import os\n",
        "os.environ['GEOMSTATS_BACKEND'] = 'pytorch'\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "from edit_distance.task.dataset_generator_genomic import EditDistanceGenomicDatasetGenerator\n",
        "from util.data_handling.data_loader import get_dataloaders\n",
        "from edit_distance.train import load_edit_distance_dataset,train,test\n",
        "from edit_distance.models.pair_encoder import PairEmbeddingDistance\n",
        "\n",
        "class LinearEncoder(nn.Module):\n",
        "    \"\"\"  Linear model which simply flattens the sequence and applies a linear transformation. \"\"\"\n",
        "\n",
        "    def __init__(self, len_sequence, embedding_size, alphabet_size=4):\n",
        "        super(LinearEncoder, self).__init__()\n",
        "        self.encoder = nn.Linear(in_features=alphabet_size * len_sequence, \n",
        "                                 out_features=embedding_size)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # flatten sequence and apply layer\n",
        "        B = sequence.shape[0]\n",
        "        sequence = sequence.reshape(B, -1)\n",
        "        emb = self.encoder(sequence)\n",
        "        return emb\n",
        "\n",
        "def run_model(dataset_name, embedding_size, dist_type, string_size, n_epoch):\n",
        "    device = 'cpu'\n",
        "    torch.manual_seed(2021)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed(2021)\n",
        "\n",
        "    # load data\n",
        "    datasets = load_edit_distance_dataset(dataset_name)\n",
        "    loaders = get_dataloaders(datasets, batch_size=128, workers=0)\n",
        "\n",
        "    # model, optimizer and loss\n",
        "\n",
        "    encoder = LinearEncoder(string_size, embedding_size)\n",
        "\n",
        "    model = PairEmbeddingDistance(embedding_model=encoder, distance=dist_type,scaling=True)\n",
        "    loss = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad() \n",
        "\n",
        "\n",
        "    # training\n",
        "    for epoch in range(0, n_epoch):\n",
        "        t = time.time()\n",
        "        loss_train = train(model, loaders['train'], optimizer, loss, device)\n",
        "        loss_val = test(model, loaders['val'], loss, device)\n",
        "\n",
        "        # print progress\n",
        "        \n",
        "        if epoch % 5 == 0:\n",
        "            print('Epoch: {:02d}'.format(epoch),\n",
        "                'loss_train: {:.6f}'.format(loss_train),\n",
        "                'loss_val: '.format(loss_val),\n",
        "                'time: {:.4f}s'.format(time.time() - t))\n",
        "        \n",
        "    # testing\n",
        "    for dset in loaders.keys():\n",
        "        avg_loss = test(model, loaders[dset], loss, device)\n",
        "        print('Final results {}: loss = {:.6f}'.format(dset, avg_loss))\n",
        "\n",
        "    return model, avg_loss\n",
        "\n",
        "def create_parser(out, source, train,val,test):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--out', type=str, default=out, help='Output data path')\n",
        "    parser.add_argument('--train_size', type=int, default=train, help='Training sequences')\n",
        "    parser.add_argument('--val_size', type=int, default=val, help='Validation sequences')\n",
        "    parser.add_argument('--test_size', type=int, default=test, help='Test sequences')\n",
        "    parser.add_argument('--source_sequences', type=str, default=source, help='Sequences data path')\n",
        "    return parser\n",
        "\n",
        "\n",
        "def generate_datasets(parser):\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    # load and divide sequences\n",
        "    with open(args.source_sequences, 'rb') as f:\n",
        "        L = f.readlines()\n",
        "    L = [l[:-1].decode('UTF-8') for l in L]\n",
        "\n",
        "    strings = {\n",
        "        'train': L[:args.train_size],\n",
        "        'val': L[args.train_size:args.train_size + args.val_size],\n",
        "        'test': L[args.train_size + args.val_size:args.train_size + args.val_size + args.test_size]\n",
        "    }\n",
        "\n",
        "    data = EditDistanceGenomicDatasetGenerator(strings=strings)\n",
        "    data.save_as_pickle(args.out)\n",
        "\n",
        "    return strings\n",
        "\n",
        "string_size=153\n",
        "n_epoch = 10\n",
        "e_size=np.logspace(1,9,num=9-1, base=2,endpoint=False, dtype=int)\n",
        "\n",
        "# dist_types=['hyperbolic', 'euclidean', 'square', 'manhattan', 'cosine']\n",
        "dist_types = ['hyperbolic']\n",
        "\n",
        "model, avg_loss = np.zeros((len(dist_types),len(e_size)),dtype=object),np.zeros((len(dist_types),len(e_size)))\n",
        "\n",
        "names = ['largest_group_strings', 'strings_test', 'strings_subset','clean_strings']\n",
        "\n",
        "for name in names:\n",
        "    dataset_name = '\\content\\' + name+'.pkl'\n",
        "\n",
        "    for i in range(len(dist_types)):\n",
        "        for j in range(len(e_size)):\n",
        "\n",
        "            print(dist_types[i])\n",
        "\n",
        "            model[i][j], avg_loss[i][j] = run_model(dataset_name,e_size[j],dist_types[i],string_size,n_epoch)\n",
        "\n",
        "    pickle.dump((model,avg_loss,e_size,dist_types), open('\\content\\'+name+'.pkl', \"wb\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "PBUMOh2r29fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "## Synthetic Dataset Results\n",
        "\n",
        "Please access the figures at these links below!\n",
        "\n",
        "\n",
        "https://github.com/nongiga/NeuroSEED/blob/master/Figure%202A.png\n",
        "Figure 2A Comparison of Loss over Embedding Dimensions - Strings_Test\n",
        "\n",
        "https://github.com/nongiga/NeuroSEED/blob/master/Figure%202B.png\n",
        "Figure 2B Raw MSE Loss Data for Each Embedding Model\n",
        "\n",
        "\n",
        "\n",
        "The data that we collected demonstrates a much lower loss for hyperbolic embedding than Euclidean embeddings across all embedding dimensions. Hyperbolic embeddings also demonstrate lower loss than all other commonly used methods of embedding aside from embedding using cosine distance for dimensions less than or equal to 4. Hyperbolic embeddings outcompete all other embedding models for higher dimensional embeddings. Further research is needed to explain the difference in using cosine embedding models.\n",
        "\n",
        "This builds upon previous NeuroSEED research (Corso et al, 2021) which does not contain the discrepancy in cosine embeddings in low dimensions. More research is needed to explain why this occurs in the specific instance of genetic sequence interpretation.\n",
        "\n",
        "### Benchmark Comparison\n",
        "\n",
        "https://github.com/nongiga/NeuroSEED/blob/master/Figure%203.png\n",
        "Figure 3: Comparison of Average Losses Between Embeddings compared to Levenshtein Distances\n",
        "\n",
        "Standard models of embedding for genetic strings use Euclidean distance. Here, we illustrate that Euclidean models fare far worse than hyperbolic embeddings in accurately estimating the distance between strings. We also provide comparisons for other commonly used distances. Interestingly, Manhattan distances perform remarkably well. One explanation is that the number of mutations necessary to move from one smORF to the next can be closely approximated with a Manhattan-like calculation (e.g. AAA to TTT has a distance of 3 Manhattan-wise and requires at minimum 3 mutations to do so). Further verification is necessary to explain this phenomenon fully.\n"
      ],
      "metadata": {
        "id": "I5z-OvOQ3acx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Real-World Applications\n",
        "\n",
        "## smORF Discovery\n",
        "\n",
        "There are a litany of smORFs that have yet to be properly classified. smORFs comprise an enormous component of cellular function, and play a vital role in nearly every major process, but remain under-targeted in research because it is very difficult to classify and study their function. Our method provides a more efficient path to classifying smORFs.\n",
        "\n",
        "## Hyperbolic Embedding Framework\n",
        "This framework also provides the groundwork for a method of comparing non-smORF proteins. In particular, our embedding method reveals the ability to partially reconstruct ancestors of existing smORF sequences and understand which mutations exist now. This provides a window into the past where we can understand how small mutations may have impacted the function of an ancestral smORF to give rise to the present diversity within a class of smORFs. More broadly, we can extend this power for longer proteins, magnifying efficiency gains compared to traditional Euclidean techniques. In doing so, we give rise to predictive power in synthesizing artificial biological tools in research and in clinic. Furthermore, our hyperbolic embedding framework provides a general power capacity to compare other hierarchical data beyond genetic sequences more efficiently.\n",
        "\n",
        "# Limitations\n",
        "A keras tuner-like function that automatically samples across hyperparameters and learning models would be beneficial to test across a variety of models during experimentation.\n",
        "\n",
        "Furthermore, we would have liked to make the encoder model less of a black box. One key result of the model is the possibility to understand ancestral relationships within clusters (e.g. region X in any sequence is more prone to A → C mutation than an A → T at position x1). These relationships can be hidden within the encoder within the weights between locally grouped neurons. The understanding of which particular regions are more likely to be conserved and which are more likely to be mutated in which ways is a key result that would be beneficial to the implications of the projects.\n",
        "\n",
        "Given the quantity of smORF data, we would have liked to use some stronger form of Recurrent Neural Network to take advantage of memory gains. For example, past research has demonstrated that LSTM networks are more effective at learning large quantities of time-series EEG data. Although the structure of such data is not hierarchical, the patterns of brain waves being determined by neighboring patterns can be similarly modeled by our embedding model. Consequently, we expect that using an LSTM architecture may advance the accuracy of our model.\n",
        "\n",
        "In the future, we hope to adapt this model to learn directly from amino acid mutations, and demonstrate that the encoder learns on amino acids, in order to further validate our results. \n"
      ],
      "metadata": {
        "id": "XP-FLcz55ojX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "Chamberlain, B.P., Clough, J.R., & Deisenroth, M.P. (2017). Neural Embeddings of Graphs in Hyperbolic Space. ArXiv, abs/1705.10359.\n",
        "\n",
        "Corso, Gabriele et al. “Neural Distance Embeddings for Biological Sequences.” ArXiv abs/2109.09740 (2021): n. Pag.\n",
        "\n",
        "Durrant MG, Bhatt AS. Automated Prediction and Annotation of Small Open Reading Frames in Microbial Genomes. Cell Host Microbe. 2021;29: 121–131.e4. Pmid:33290720\n",
        "\n",
        "Gray T, Storz G, Papenfort K. Small Proteins; Big Questions. J Bacteriol. 2022 Jan 18;204(1):e0034121. doi: 10.1128/JB.00341-21. Epub 2021 Jul 26. PMID: 34309401; PMCID: PMC8765408.\n",
        "\n",
        "H. Sberro, B.J. Fremin, S. Zlitni, F. Edfors, N. Greenfield, M.P. Snyder, G.A. Pavlopoulos, N.C. Kyrpides, A.S. Bhatt\n",
        "Large-scale analyses of human microbiomes reveal thousands of small, novel genes\n",
        "Cell, 178 (2019), pp. 1245-1259.e14\n",
        "\n"
      ],
      "metadata": {
        "id": "W53k0JM95wnu"
      }
    }
  ]
}
