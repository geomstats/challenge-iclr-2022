{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takes my smorf dataset and uses NeuroSEED's code to store it in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from scipy.stats import mode\n",
    "import argparse\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from edit_distance.task.dataset_generator_genomic import EditDistanceGenomicDatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset as string\n",
    "df = pd.read_csv(\"../datasets/dataset_FINAL.tsv\", sep='\\t')\n",
    "\n",
    "file_out='../datasets/strings.txt'\n",
    "\n",
    "no_weird_chars=df['smorf'].str.contains(r'^[ACTG]+$', na=False)\n",
    "\n",
    "with open(file_out, 'w') as f_out:\n",
    "    f_out.writelines(\"%s\\n\" % l for l in df[no_weird_chars].smorf.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edit distance 7000x7000:  55%|█████▍    | 3848/7000 [02:24<01:53, 27.88it/s]"
     ]
    }
   ],
   "source": [
    "# create string to be read by NeuroSEED-borrowed code\n",
    "file_out='../datasets/strings.txt'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--out', type=str, default=\"../datasets/strings.pkl\", help='Output data path')\n",
    "parser.add_argument('--train_size', type=int, default=20000, help='Training sequences')\n",
    "parser.add_argument('--val_size', type=int, default=2000, help='Validation sequences')\n",
    "parser.add_argument('--test_size', type=int, default=4000, help='Test sequences')\n",
    "parser.add_argument('--source_sequences', type=str, default=file_out, help='Sequences data path')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# load and divide sequences\n",
    "with open(args.source_sequences, 'rb') as f:\n",
    "    L = f.readlines()\n",
    "L = [l[:-1].decode('UTF-8') for l in L]\n",
    "\n",
    "strings = {\n",
    "    'train': L[:args.train_size],\n",
    "    'val': L[args.train_size:args.train_size + args.val_size],\n",
    "    'test': L[args.train_size + args.val_size:args.train_size + args.val_size + args.test_size]\n",
    "}\n",
    "\n",
    "data = EditDistanceGenomicDatasetGenerator(strings=strings)\n",
    "data.save_as_pickle(args.out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'val', 'test'])\n"
     ]
    }
   ],
   "source": [
    "with open('../datasets/string_subset.pkl', 'rb') as f:\n",
    "        sequences, distances = pickle.load(f)\n",
    "\n",
    "print(distances.keys())\n",
    "slices={'train':100, 'test':10, 'val':20}\n",
    "smaller_distances = {key: distances[key][:slices[key],:slices[key]] for key in distances.keys()}\n",
    "smaller_sequences = {key: sequences[key][:slices[key]] for key in sequences.keys()}\n",
    "\n",
    "pickle.dump((smaller_sequences, smaller_distances),open('../datasets/string_for_test' + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edit distance 50x50: 100%|██████████| 50/50 [00:00<00:00, 2955.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute the matrix: 0.02004694938659668\n",
      "Generating val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Edit distance 7x7: 100%|██████████| 7/7 [00:00<00:00, 12504.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute the matrix: 0.003579378128051758\n",
      "Generating test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Edit distance 4x4: 100%|██████████| 4/4 [00:00<00:00, 82646.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compute the matrix: 0.0031969547271728516\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "df = pd.read_csv(\"../datasets/dataset_FINAL.tsv\", sep='\\t')\n",
    "smorfams=df.clust[df.clust.str.startswith('smorfam') & df.y.str.fullmatch('positive')]\n",
    "md,count=mode(smorfams)\n",
    "no_weird_chars=df['smorf'].str.contains(r'^[ACTG]+$', na=False)\n",
    "\n",
    "to_write=df.clust.str.startswith(md[0]) & df.y.str.fullmatch('positive') & no_weird_chars\n",
    "\n",
    "file_out='../datasets/largest_group_strings.txt'\n",
    "\n",
    "with open(file_out, 'w') as f_out:\n",
    "    f_out.writelines(\"%s\\n\" % l for l in df[to_write].smorf.values)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--out', type=str, default=\"../datasets/largest_group_strings.pkl\", help='Output data path')\n",
    "parser.add_argument('--train_size', type=int, default=50, help='Training sequences')\n",
    "parser.add_argument('--val_size', type=int, default=7, help='Validation sequences')\n",
    "parser.add_argument('--test_size', type=int, default=7, help='Test sequences')\n",
    "parser.add_argument('--source_sequences', type=str, default=file_out, help='Sequences data path')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# load and divide sequences\n",
    "with open(args.source_sequences, 'rb') as f:\n",
    "    L = f.readlines()\n",
    "L = [l[:-1].decode('UTF-8') for l in L]\n",
    "\n",
    "strings = {\n",
    "    'train': L[:args.train_size],\n",
    "    'val': L[args.train_size:args.train_size + args.val_size],\n",
    "    'test': L[args.train_size + args.val_size:args.train_size + args.val_size + args.test_size]\n",
    "}\n",
    "\n",
    "data = EditDistanceGenomicDatasetGenerator(strings=strings)\n",
    "data.save_as_pickle(args.out)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95cf8f9d78f9a4efc159d15b812704439377ddaf89110be74698069e8aea9009"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('neuroseed': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
